All questions in the notebook have been completed with concise answers covering:

Q2: SST-2 dataset, confidence scores (0-1), emotion classification models  
Q3: Token aggregation, entity types, subword tokenization, CoNLL-2003 dataset  
Q4: Extractive QA, span indices, SQuAD dataset, generative vs extractive differences  
Q5: Abstractive vs extractive summarization, BART architecture, length parameters  
Q6: MarianMT architecture, OPUS corpus, bilingual vs multilingual models, M2M100  
Q7: GPT-2 architecture (124M params), generation parameters (temperature, top_k), reproducibility  
